E.1 Tokenization

Tokenization divides a text into smaller units called tokens. These tokens can be indi-
vidual words, sub-words, characters, or even complete phrases, depending on the level of
granularity desired to obtain. Generally, these divisions are usually given by sub-words
in LLM context.

For example, the sentence “Don’t waste my time. I’m the fastest.”. could be encoded as

[Do] [n’t] [waste] [my] [time] [.]
Another possible tokenization processes can be: