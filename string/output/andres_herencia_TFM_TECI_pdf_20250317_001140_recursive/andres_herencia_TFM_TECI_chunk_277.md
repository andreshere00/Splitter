Once the tokens are obtained, they are vectorized. Vectorization allows each token to be
represented as a vector in an n-dimensional space, where n âˆˆ N is the number of features.
This allows simple dependency or similarity relationships to be established between words.