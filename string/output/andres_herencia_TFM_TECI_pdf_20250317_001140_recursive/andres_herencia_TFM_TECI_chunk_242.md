Based on the results, the fine-tuning process did not significantly improve the quality of
either the base model or reference responses. This can be attributed to several factors:
the relatively small size of the model (only 7 billion parameters), the quantization con-
figuration (which slightly compromises precision and accuracy metrics compared to the
non-quantized model), and the limited available data, considering that LLaMA 2 was