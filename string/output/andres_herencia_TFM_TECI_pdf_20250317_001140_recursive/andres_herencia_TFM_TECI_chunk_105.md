In the convolution or recurrence mechanisms absence, a technique that implements the
required ordering is needed. Therefore, its relative or absolute position is added during
the embedding generation. Both the embedding and its position are encoded onto vectors
of the same dimension, d = 512, so, they can operate together. Without positional embed-
ding, the representation would be permutation-invariant and the model would perceive
sequences as “bags of words” instead. [50]