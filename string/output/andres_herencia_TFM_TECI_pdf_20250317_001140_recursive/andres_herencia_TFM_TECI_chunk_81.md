2.4 Architectures with attention mechanisms

Attention mechanisms allow models to focus on a specific text part, avoiding memory
problems. Conceptually, it can be understood as a resource allocation scheme, so, a
priori, it can solve the information bottleneck problem mentioned above.

2.4.1 RNNSearch

RNNsearch responds to an encoder-decoder architecture based on Bidirectional Recurrent
Neural Networks (BiRNN) in the encoder and an attention-based decoder.

Encoder