Note that there is a hybrid attention mechanism called encoder-decoder attention, or
cross-attention. This mechanism is analogous to self-attention but utilizes keys and values
from the encoder and queries from the decoder. It combines contextual information from
the encoder with task-specific information from the decoderâ€™s previous outputs. This
approach is similar to the attention block in the RNNSearch architecture discussed in
section 2.4. Refer to Figure 3.1.