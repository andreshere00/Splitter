h = 512

19

3.3. ATTENTION MECHANISMS

The purpose of paralleling the attention mechanism via multiple heads is solely to reduce
computational complexity during the training phase (which states for a highly efficient
architecture compared with recurrent big-data processing architectures).

A simplified diagram describing the Multi-Head Attention mechanism can be found

in Figure 3.4. [42]

Figure 3.4: Multi-Head Attention mechanism.

Cross-attention mechanism