initially pre-trained with a dataset up to 2 trillion tokens. Additionally, the available
computational resources (virtual machines) were insufficient for more in-depth analyses,
precluding the use of larger models and big data sources.