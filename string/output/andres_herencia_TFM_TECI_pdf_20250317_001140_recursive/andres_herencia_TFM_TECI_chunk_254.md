• Fine-tuning with larger databases: A training process with a larger database

could ease the model adaptation to this specific domain.

• Training without quantization: Since quantization shows slightly worse perfor-

mance in inference and training tasks, tests could be conducted without it.

• Use of other models, with empiric better results in text generation tasks, such

as GPT [51] or the novel LLaMA version. [40]