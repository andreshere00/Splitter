. . . . . . 12

2.8 A representation of the attention weights in a text extract. [44]

. . . . . . 13

3.1 Transformer architecture diagram. [61]

. . . . . . . . . . . . . . . . . . . . 14

3.2 Embedding generation process . . . . . . . . . . . . . . . . . . . . . . . . . 17

3.3 Scaled-dot product self-attention mechanism. . . . . . . . . . . . . . . . . . 19

3.4 Multi-Head Attention mechanism. . . . . . . . . . . . . . . . . . . . . . . . 20

3.5 LLaMA architecture diagram.