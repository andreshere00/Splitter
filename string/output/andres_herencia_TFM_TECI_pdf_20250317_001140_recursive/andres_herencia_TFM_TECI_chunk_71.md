To solve this problem, several recurrent architectures were raised, such as Long short-
term memory (LSTM) cells and Gated Recurrent Unit (GRU) cells (these architectures
can be consulted on appendices C and D, respectively). These models tried to avoid
the gradient vanishing problem using memory mechanisms. However, the most relevant
architecture that has led to improved performance in machine translation tasks was the
encoder-decoder.

2.3 Encoder-decoder architecture