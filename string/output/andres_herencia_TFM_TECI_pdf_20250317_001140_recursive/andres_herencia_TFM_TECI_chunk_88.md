This approach distributes the information from the source sentence across the entire
sequence, rather than encoding it all into a fixed-length vector through the encoder.
Meanwhile, the decoder can selectively access this information at each time step. This
formulation allows the neural network to concentrate on pertinent elements of the input
rather than irrelevant ones (the actual attention concept). An architecture diagram is