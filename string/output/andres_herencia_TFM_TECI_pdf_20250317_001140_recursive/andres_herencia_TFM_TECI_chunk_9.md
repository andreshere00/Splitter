The rapid advances of generative Artificial Intelligence (AI) have marked a milestone
in the Natural Language Processing (NLP) field. Specifically, Transformer models have
revolutionized the state-of-the-art due to their great effectiveness and efficiency in several
tasks, both general and specific. Thus, this work explores the Transformer architecture
and its application to Large Language Models (LLMs) through Parameter-Efficient Fine