In 2017 Google’s paper [61], asses the following statement: «We chose this function be-
cause we hypothesized it would allow the model to easily learn to attend by relative
positions, since for any fixed offset k, P Epos+k can be represented as a linear function of

XIII

F.2. RELATIVE POSITION ENCODING

Table F.1: Positional Embeddings. Extracted from own-made Python script.

Position Index 0
0.0000
0.8415
0.9093
0.1411
-0.7568
-0.9589
-0.2794
0.6570
0.9894
0.4121

0
1
2
3
4
5
6
7
8
9