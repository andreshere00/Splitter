Therefore, Parameter-Efficient Fine Tunning (PEFT) techniques are generally em-
ployed. Some methods include using adapter layers, quantization methods, prefix and
prompt tuning (a form of prompt engineering), or hybrid or novel state-of-the-art meth-
ods. For domain-specific adaptation, where the base model has billions of parameters
and high computational complexity, Quantized Low Rank Adapters (QLoRA) is used.
QLoRA combines quantization and low-rank adapters to reduce computational demands