The RNN conventional architecture presents significant challenges, notably the gra-
dient vanishing and explosion problems. As detailed in [57], small gradients diminish
exponentially as they propagate back through time, leading to the vanishing or fading
gradient issue. Conversely, very high gradients can increase exponentially, resulting in an
explosion problem. Both scenarios hinder the effective training of the network.