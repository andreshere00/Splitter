LLMs utilize parametric memory, where the model’s knowledge is encoded within its
training parameters. Consequently, a higher number of parameters equates to more stored
information. However, this also means LLMs cannot access external information sources
beyond their training data, potentially leading to inaccuracies and “hallucinations” [24]
on certain topics. To address this limitation, Retrieval-Augmented Generation (RAG)
systems have been developed.

Architecture