This chapter presents the Transformer model, the first encoder-decoder architecture
that relies exclusively on self-attention mechanisms to process sequential data, unlike its
predecessors that relied fundamentally on recurrent structures. Disregarding recurrence in
its architecture, computational efficiency and machine translation task performance have
been improved, avoiding the previously mentioned training problems. Figure 3.1 depicts