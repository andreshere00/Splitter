Training Transformer-based models are quite expensive and require a vast amount of
data. Therefore, pre-trained models are often used for inference tasks. Consequently, the
question of how can a pre-trained model be modified and adapted to perform specific
tasks required by the user arises. To achieve this, state-of-the-art methods suggest three
techniques for tuning models: (i) Prompt Engineering, (ii) Fine-tuning, and (iii) Retrieval-
Augmented Generation (RAG). Below will be presented.