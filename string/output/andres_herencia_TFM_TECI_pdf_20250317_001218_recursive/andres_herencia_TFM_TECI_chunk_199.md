[32] It is an open-source Google-developed
tool that implements diverse tokenization techniques, generally via sub-words (con-
sult this concept in appendix E.1).
It is the base model default tokenizer. The
special tokens from the data format have been added.

5.2.2 Training parameters

Generally, in classical machine learning training, exhaustive hyper-parameter search meth-
ods are conducted, such as the Grid Search [33] or the Bayesian Hyper-parameter tuning