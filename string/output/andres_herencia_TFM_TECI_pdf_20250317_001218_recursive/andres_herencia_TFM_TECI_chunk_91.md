Successive improvements emerged based on this attention mechanism (such as the unified
attention model) which moderately improved the performance of machine translation
tasks. However, performance was limited by the use of recurrence-based architectures in
the models. The fact is that, although order and dependencies between input vectors are
captured, it makes training a process hardly parallelizable. Furthermore, it limits the