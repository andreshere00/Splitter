−1,

if x ≥ 0

if x < 0

but other functions are used in different contexts and models. The most important ones
are explained below.(observe Figure 2.2). [54]:

• Sigmoid-shaped functions: σ(x) =

1

1−e−αx, with α ∈ R, and being x a scalar

number.

• Hyper parabolic tangent function: tanh (x) = sinh (x)
cosh (x)

.

• Rectified Linear Unit: ReLU(x) = max (0, x) used in other contexts and neural

network types.

• Softmax function: Used in multi-classification contexts, softmax = σ(zj) =